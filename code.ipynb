{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from wfdb.processing import resample_multichan, find_local_peaks, compare_annotations\n",
    "\n",
    "from scipy.signal import convolve\n",
    "\n",
    "from wfdb import wrann, rdann\n",
    "\n",
    "from io import StringIO\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dropout, Dense, TimeDistributed, LSTM, Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import mean_squared_error\n",
    "from tensorflow.keras.metrics import mean_absolute_error\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('Unpreprocessed_Data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parabola(a,n,r = 4): \n",
    "    '''    \n",
    "    n : dimension of the segment to held the parables\n",
    "    a : list object of the positions of the QRS complexes\n",
    "    r : half amplitude of the parables, that is 4 by default\n",
    "    \n",
    "    '''\n",
    "    assert n>2*r\n",
    "    \n",
    "    y = np.zeros(n, dtype = np.float32)\n",
    "    \n",
    "    x= np.array(range(2,2*r+1))\n",
    "    \n",
    "    for i in a:\n",
    "        \n",
    "        if i > r-1 and i <= n-r:\n",
    "            \n",
    "            y[i-r+1:i+r] = ((r+1)**2-(x-r-1)**2)/(r+1)**2\n",
    "            \n",
    "        elif i < r:\n",
    "            \n",
    "            y[:i+r] = ((r+1)**2-(x[r-i-1:]-r-1)**2)/(r+1)**2\n",
    "        \n",
    "        elif i<n:\n",
    "            \n",
    "            y[i-r+1:] = ((r+1)**2-(x[:r-1+(n-i)]-r-1)**2)/(r+1)**2\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Identifier_list = list()\n",
    "\n",
    "for element in range(len(os.listdir())):\n",
    "    \n",
    "    Identifier_list.append(os.listdir()[element][0:3])\n",
    "    \n",
    "Identifier_list = list(dict.fromkeys(Identifier_list))\n",
    "\n",
    "\n",
    "for _id in Identifier_list:\n",
    "    \n",
    "    if _id.startswith('I'):\n",
    "        \n",
    "        channels = ['II', 'V1']\n",
    "        \n",
    "        file_type = 'Training File'\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        channels = ['MLII', 'V5']\n",
    "        \n",
    "        file_type = 'Testing File'\n",
    "        \n",
    "    \n",
    "    Annot = wfdb.rdann(_id, extension = 'atr')\n",
    "    \n",
    "    Sig, Head = wfdb.rdsamp(_id, channel_names = channels)\n",
    "        \n",
    "    Sig, Annot = resample_multichan(Sig, Annot, Head['fs'], 100) \n",
    "    \n",
    "    Weights = np.repeat(1.0, 100) / 100 \n",
    "        \n",
    "    Conv_Sig_0 = convolve(Sig[:,0], Weights, mode = 'same') \n",
    "    Conv_Sig_1 = convolve(Sig[:,1], Weights, mode = 'same') \n",
    "        \n",
    "    Final_Sig_0 = Sig[:,0] - Conv_Sig_II \n",
    "    Final_Sig_1 = Sig[:,1] - Conv_Sig_V1\n",
    "        \n",
    "    Final_Sig = np.vstack([Final_Sig_0, Final_Sig_1])\n",
    "    Final_Sig = np.transpose(Final_Sig)\n",
    "    \n",
    "    signal_list = ['N', 'L', 'R', 'B', 'A', 'a', 'J', 'S', 'V', 'r', 'F', 'e', 'j', 'n', 'E', '/', 'f', 'Q', '?']\n",
    "    QRS_Index = [pos for pos, signal in enumerate(Annot.symbol) if signal in signal_list] \n",
    "    QRS_Pos = [int(list(Annot.sample)[i]) for i in QRS_Index] \n",
    "        \n",
    "    Target_Seq = parabola(QRS_Pos, Sig.shape[0]) \n",
    "        \n",
    "    Sig_Target = np.insert(Final_Sig, 2, Target_Seq, axis = 1) \n",
    "        \n",
    "    np.save(str(_id), Sig_Target)\n",
    "    \n",
    "    print(file_type + _id + ' done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('Preprocessed_Data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Training_records, Testing_records = 75, 48\n",
    "\n",
    "Training_array = np.zeros((int(Training_records * 0.75) + 2, 3, 180000), dtype = np.float32)\n",
    "\n",
    "Validation_array = np.zeros((int(Training_records * 0.25) + 1, 3, 180000), dtype = np.float32)\n",
    "\n",
    "Testing_array = np.zeros((Testing_records, 3, 180555), dtype = np.float32 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_count, test_count = 0, 0\n",
    "\n",
    "for _id in range(len(os.listdir())):\n",
    "    \n",
    "    file = os.listdir()[_id]\n",
    "    \n",
    "    if file.startswith('I'):\n",
    "        \n",
    "        record = np.transpose(np.load(file))\n",
    "        \n",
    "        if train_count <= Validation_array.shape[0] - 1:\n",
    "        \n",
    "            Validation_array[train_count,:,:] = record\n",
    "            train_count += 1\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            Training_array[train_count - 18,:,:] = record\n",
    "            train_count += 1\n",
    "    else:\n",
    "        \n",
    "        record = np.transpose(np.load(file))\n",
    "        Testing_array[test_count,:,:] = record\n",
    "        \n",
    "        test_count += 1       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Select_ECG_portion(array_3D, seqL, ninputs):\n",
    "    \n",
    "    record_id = tf.random.uniform([1], minval = 0, maxval = array_3D.shape[0], dtype = tf.dtypes.int32)\n",
    "    \n",
    "    array_2D = array_3D[record_id[0],:,:]\n",
    "        \n",
    "    record_seg_dim = seqL * ninputs\n",
    "    \n",
    "    record_seg = tf.random_crop(array_2D, [3, record_seg_dim])\n",
    "    \n",
    "    inputs_ch1 = record_seg[0,:]\n",
    "    inputs_ch2 = record_seg[1,:]\n",
    "    \n",
    "    inputs = tf.concat([inputs_ch1, inputs_ch2], axis = 0)\n",
    "    target = record_seg[2,:]\n",
    "    \n",
    "    return inputs, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqL, fs, time_step = 20, 100, 0.5\n",
    "\n",
    "ninputs = int(time_step * fs)\n",
    "\n",
    "batchSize = 8\n",
    "\n",
    "Train = tf.data.Dataset.from_tensor_slices(Training_array)\n",
    "Train = Train.map(lambda x:  Select_ECG_portion(x,seqL, ninputs))\n",
    "Train = Train.repeat()\n",
    "Train = Train.batch(batchSize)\n",
    "\n",
    "Validation = tf.data.Dataset.from_tensor_slices(Validation_array)\n",
    "Validation = Validation.map(lambda x:  Select_ECG_portion(x,seqL, ninputs))\n",
    "Validation = Validation.repeat()  \n",
    "Validation = Validation.batch(batchSize)\n",
    "\n",
    "Test = tf.data.Dataset.from_tensor_slices(Testing_array)\n",
    "Test = Test.map(lambda x:  Select_ECG_portion(x,seqL, ninputs))\n",
    "Test = Test.repeat()  \n",
    "Test = Test.batch(batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dense_units = 2000\n",
    "\n",
    "ff_Model = Sequential()\n",
    "\n",
    "ff_Model.add(Dense(Dense_units, activation = 'relu',input_shape = (2*seqL*ninputs,)))\n",
    "\n",
    "ff_Model.add(Dense(Dense_units, activation = 'relu'))\n",
    "ff_Model.add(Dense(Dense_units, activation = 'relu'))\n",
    "ff_Model.add(Dense(Dense_units, activation = 'relu'))\n",
    "ff_Model.add(Dense(Dense_units, activation = 'relu'))\n",
    "\n",
    "ff_Model.add(Dense(seqL*ninputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ff_Model.compile(optimizer = Adam(0.001), loss = mean_squared_error, metrics = [ mean_absolute_error ])\n",
    "ff_Model.fit(Train, epochs = 90, steps_per_epoch = 1500, validation_data = Validation, validation_steps = 100, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = ff_Model.evaluate(Test, steps = 1000)\n",
    "\n",
    "print('test mean square error (loss): ', out[0], '  test absolute error: ', out[1])\n",
    "\n",
    "iterator = Test.make_initializable_iterator()\n",
    "\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(iterator.initializer)\n",
    "    \n",
    "    inp, targ = sess.run(next_element)\n",
    "    \n",
    "output = ff_Model.predict(inp)\n",
    "\n",
    "t = range(1000)\n",
    "\n",
    "plt.plot(t,inp[5,:1000],'k',t,targ[5,:1000]-2,'r',t,output[5,:1000]-2,'b')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqL, fs, time_step = 35, 100, 0.3\n",
    "\n",
    "ninputs = int(time_step * fs)\n",
    "\n",
    "batchSize = 8\n",
    "\n",
    "Train = tf.data.Dataset.from_tensors(Training_array)\n",
    "Train = Train.map(lambda x:  Select_ECG_portion(x,seqL, ninputs))\n",
    "Train = Train.repeat()\n",
    "Train = Train.batch(batchSize)\n",
    "\n",
    "Validation = tf.data.Dataset.from_tensors(Validation_array)\n",
    "Validation = Validation.map(lambda x:  Select_ECG_portion(x,seqL, ninputs))\n",
    "Validation = Validation.repeat()  \n",
    "Validation = Validation.batch(batchSize)\n",
    "\n",
    "Test = tf.data.Dataset.from_tensors(Testing_array)\n",
    "Test = Test.map(lambda x:  Select_ECG_portion(x,seqL, ninputs))\n",
    "Test = Test.repeat()  \n",
    "Test = Test.batch(batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_units = 212\n",
    "\n",
    "denseDim = ninputs\n",
    "\n",
    "\n",
    "rr_Model = tf.keras.Sequential()\n",
    "\n",
    "rr_Model.add(Reshape((seqL,2*ninputs), input_shape = (2*seqL*ninputs,)))\n",
    "\n",
    "rr_Model.add(LSTM(units = LSTM_units, return_sequences = True))\n",
    "rr_Model.add(LSTM(units = LSTM_units, return_sequences = True))\n",
    "rr_Model.add(LSTM(units = LSTM_units, return_sequences = True))\n",
    "rr_Model.add(LSTM(units = LSTM_units, return_sequences = True))\n",
    "rr_Model.add(LSTM(units = LSTM_units, return_sequences = True))\n",
    "\n",
    "rr_Model.add(TimeDistributed(Dense(denseDim)))\n",
    "\n",
    "rr_Model.add(Reshape((seqL*ninputs,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rr_Model.compile(optimizer = Adam(0.01), loss = mean_squared_error,metrics = [mean_absolute_error])\n",
    "rr_Model.fit(Train, epochs = 130, steps_per_epoch = 1500, validation_data = Validation, validation_steps = 100, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = rr_Model.evaluate(Test, steps = 1000)\n",
    "\n",
    "print('test mean square error (loss): ', out[0], '  test absolute error: ', out[1])\n",
    "\n",
    "iterator = Test.make_initializable_iterator()\n",
    "\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(iterator.initializer)\n",
    "    \n",
    "    inp, targ = sess.run(next_element)\n",
    "    \n",
    "output = rr_Model.predict(inp)\n",
    "\n",
    "t = range(1000)\n",
    "\n",
    "plt.plot(t,inp[7,:1000],'k',t,targ[7,:1000]-2,'r',t,output[7,:1000]-2,'b')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posprocessing Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_dict_ff = dict()\n",
    "Test_dict_rr = dict()\n",
    "\n",
    "for _id in range(len(os.listdir())):\n",
    "    \n",
    "    file = os.listdir()[_id]\n",
    "    \n",
    "    if not file.startswith('I'):\n",
    "        \n",
    "        record = np.transpose(np.load(file))\n",
    "        \n",
    "        Test_dict_ff[file] = [record[:2,:], record[2,:]]\n",
    "        Test_dict_rr[file] = [record[:2,:], record[2,:]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(array_2D, seqL, ninputs):\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    record_seg_dim = int(seqL * ninputs)\n",
    "    \n",
    "    offset = 0\n",
    "    \n",
    "    for n_time in range(int(180555 / (record_seg_dim))):\n",
    "        \n",
    "        record_seg = array_2D[:, offset : offset + record_seg_dim]\n",
    "        \n",
    "        results.append(record_seg)\n",
    "        \n",
    "        offset += record_seg_dim\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_pred(crop_list, Model, Model_type):\n",
    "    \n",
    "    if Model_type == 'ff':\n",
    "        \n",
    "        dim = 2000\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        dim = 2100\n",
    "    \n",
    "    input_ = np.hstack([crop_list[0][0,:], crop_list[0][1,:]]).reshape(-1,dim)\n",
    "        \n",
    "    output = Model.predict(input_) \n",
    "    \n",
    "    for crop_seg in range(1,len(crop_list)):\n",
    "        \n",
    "        input_ = np.hstack([crop_list[crop_seg][0,:], crop_list[crop_seg][1,:]]).reshape(-1,dim)\n",
    "        \n",
    "        output_ = Model.predict(input_)\n",
    "        \n",
    "        output = np.hstack([output, output_])\n",
    "        \n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_position(list_values, threshold_value):\n",
    "    \n",
    "    pred_array = list_values[0].flatten()\n",
    "    \n",
    "    ground_truth = np.array(list_values[1])\n",
    "    \n",
    "    \n",
    "    initial_pos = find_local_peaks(pred_array, 25)\n",
    "    \n",
    "    logic_vec = pred_array[initial_pos] >= threshold_value\n",
    "    \n",
    "    final_pos_pred = initial_pos[logic_vec]\n",
    "    \n",
    "    final_pos_true = np.where(ground_truth == 1)\n",
    "    \n",
    "    \n",
    "    return final_pos_pred, final_pos_true[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comparison(Test_dict, dict_type):\n",
    "    \n",
    "    for ECG_file in Test_dict.keys():\n",
    "    \n",
    "        reference = rdann(ECG_file[:3] + '_gt', extension = 'atr' )\n",
    "    \n",
    "        prediction = rdann(ECG_file[:3] + '_pr_' + dict_type , extension = 'atr')\n",
    "    \n",
    "        comparator = compare_annotations(reference.sample, prediction.sample, 12)\n",
    "    \n",
    "        string = StringIO()\n",
    "    \n",
    "        sys.stdout = string\n",
    "    \n",
    "        comparator.print_summary()\n",
    "    \n",
    "        Test_dict[ECG_file] = [string.getvalue().split('\\n')[6], string.getvalue().split('\\n')[7] ]\n",
    "        \n",
    "    return Test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqL_ff, seqL_rr = 20, 35\n",
    "\n",
    "fs = 100\n",
    "\n",
    "time_step_ff, time_step_rr = 5, 0.3\n",
    "\n",
    "ninputs_rr, ninputs_ff = int(time_step_rr * fs), int(time_step_ff * fs)\n",
    "\n",
    "for ECG_file in Test_dict_rr.keys():\n",
    "    \n",
    "    Test_dict_rr[ECG_file][0] = crop(Test_dict_rr[ECG_file][0], seqL_rr, ninputs_rr)\n",
    "    \n",
    "    Test_dict_rr[ECG_file][0] = total_pred(Test_dict_rr[ECG_file][0], rr_Model, 'rr')\n",
    "    \n",
    "    Test_dict_rr[ECG_file] = get_position(Test_dict_rr[ECG_file], 0.5)\n",
    "    \n",
    "    Test_dict_ff[ECG_file][0] = crop(Test_dict_ff[ECG_file][0], seqL_ff, ninputs_ff)\n",
    "    \n",
    "    Test_dict_ff[ECG_file][0] = total_pred(Test_dict_ff[ECG_file][0], ff_Model, 'ff')\n",
    "    \n",
    "    Test_dict_ff[ECG_file] = get_position(Test_dict_ff[ECG_file], 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ECG_file in Test_dict_ff.keys():\n",
    "    \n",
    "    wrann(ECG_file[:3] + '_pr_ff', 'atr', Test_dict_ff[ECG_file][0], ['N'] * len(Test_dict_ff[ECG_file][0]) )\n",
    "    \n",
    "    wrann(ECG_file[:3] + '_pr_rr', 'atr', Test_dict_rr[ECG_file][0], ['N'] * len(Test_dict_rr[ECG_file][0]) )\n",
    "    \n",
    "    wrann(ECG_file[:3] + '_gt', 'atr', Test_dict_rr[ECG_file][1], ['N'] * len(Test_dict_rr[ECG_file][1]) ) \n",
    "        \n",
    "    print(ECG_file + ' done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_dict_rr = get_comparison(Test_dict_rr, 'rr')\n",
    "\n",
    "Test_dict_ff = get_comparison(Test_dict_ff, 'ff')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
